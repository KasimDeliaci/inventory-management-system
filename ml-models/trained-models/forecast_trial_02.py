# -*- coding: utf-8 -*-
"""Forecast_Trial_02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15jYUw4gpxpWW-v6ASO7U-xjFFuWwyIrx
"""

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta
import holidays
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from keras.layers import Dense, Dropout
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

df = pd.read_csv("hierarchical_sales_data.csv")
df.tail()

df["DATE"] = pd.to_datetime(df["DATE"])
df.info()

df.info()

df.set_index("DATE",inplace = True)
df.head()

df.shape

df = df.rename(columns = {"DATE":"date"})
df.head()

df = df.asfreq("D")

df.index

df = df.drop(columns = df.filter(like="QTY_B1").columns)

df = df.drop(columns = df.filter(like="QTY_B2").columns)

df = df.drop(columns = df.filter(like="QTY_B3").columns)

df = df.drop(columns = df.filter(like="PROMO_B1").columns)

df = df.drop(columns = df.filter(like="PROMO_B2").columns)

df = df.drop(columns = df.filter(like="PROMO_B3").columns)

df.info()

df = df.rename(columns = {"QTY_B4_1":"P1",
                          "QTY_B4_2":"P2",
                          "QTY_B4_3":"P3",
                          "QTY_B4_4":"P4",
                          "QTY_B4_5":"P5",
                          "QTY_B4_6":"P6",
                          "QTY_B4_7":"P7",
                          "QTY_B4_8":"P8",
                          "QTY_B4_9":"P9",
                          "QTY_B4_10":"P10",})
df.head()

df = df.rename(columns = {"PROMO_B4_1":"Promo_P1",
                          "PROMO_B4_2":"Promo_P2",
                          "PROMO_B4_3":"Promo_P3",
                          "PROMO_B4_4":"Promo_P4",
                          "PROMO_B4_5":"Promo_P5",
                          "PROMO_B4_6":"Promo_P6",
                          "PROMO_B4_7":"Promo_P7",
                          "PROMO_B4_8":"Promo_P8",
                          "PROMO_B4_9":"Promo_P9",
                          "PROMO_B4_10":"Promo_P10",})
df.head()

df.to_csv ("Sales.csv", index = False)

df.to_excel(excel_writer = "/content/Sales.xlsx" , sheet_name = "Sales.excel")

# === Parameters ===
input_file = "Sales.xlsx"   # input file
sheet_name = "Sales.excel"      # sheet name
output_file = "Sales_transformed.xlsx"  # output file

# === Load data ===
df = pd.read_excel(input_file, sheet_name=sheet_name)

# Identify columns
sales_cols = [c for c in df.columns if c.startswith("P") and not c.startswith("Promo")]
promo_cols = [c for c in df.columns if c.startswith("Promo")]

# Melt sales
sales_long = df.melt(id_vars=["DATE"], value_vars=sales_cols,
                     var_name="product_id", value_name="sales")

# Melt promos
promo_long = df.melt(id_vars=["DATE"], value_vars=promo_cols,
                     var_name="product_id", value_name="promo")

# Clean product_id names (remove "Promo_")
promo_long["product_id"] = promo_long["product_id"].str.replace("Promo_", "")

# Merge sales + promo
long_df = sales_long.merge(promo_long, on=["DATE", "product_id"])

# === Add exogenous features ===
np.random.seed(42)  # reproducibility

# Campaign type: random integers [0,2] (0 = no campaign, 1 = campaign, 2 = special offer)
long_df["campaign_type"] = np.random.randint(0, 3, size=len(long_df))

# Discount ratio & duration: only > 0 if campaign exists
def assign_discount(campaign_type):
    if campaign_type == 1:  # campaign
        return np.random.uniform(0.05, 0.2)
    elif campaign_type == 2:  # special offer
        return np.random.uniform(0.2, 0.6)
    return 0.0  # no campaign → no discount

def assign_duration(campaign_type):
    if campaign_type == 1:
        return np.random.randint(5, 10)
    elif campaign_type == 2:
        return np.random.randint(7, 15)
    return 0  # no campaign → no duration

long_df["discount_ratio"] = long_df["campaign_type"].apply(assign_discount)
long_df["campaign_duration"] = long_df["campaign_type"].apply(assign_duration)

# Holiday type: 0=workday, 1=public holiday, 2=weekend
long_df["DATE"] = pd.to_datetime(long_df["DATE"])
long_df["holiday_type"] = long_df["DATE"].dt.weekday.apply(lambda x: 2 if x >= 5 else 0)
holiday_idx = np.random.choice(long_df.index, size=int(0.05 * len(long_df)), replace=False)
long_df.loc[holiday_idx, "holiday_type"] = 1

# Consumer confidence index: normal around 100
long_df["consumer_confidence_index"] = np.random.normal(loc=100, scale=5, size=len(long_df))
long_df["consumer_confidence_index"] = long_df["consumer_confidence_index"].clip(95, 115)

# === Save transformed dataset ===
long_df.to_excel(output_file, index=False)

print(f"Transformed dataset saved to {output_file}")

df = pd.read_excel("Sales_transformed (1).xlsx")
df.to_csv("Sales_transformed.csv", index=False)
df.head()

df["DATE"] = pd.to_datetime(df["DATE"])
df.info()

df.set_index("DATE", inplace = True)
df.head()

df = df[~df.index.duplicated(keep='first')]

df = df.asfreq ("D")
df.head()

df.index

df.info()

"""#SARIMAX Model"""

df = df.sort_values(["product_id", "DATE"])

# Choose a single product for SARIMAX (SARIMAX works best on single time-series)
#product_id = "P1"
#product_df = df03[df03["product_id"] == product_id].set_index("DatetimeIndex")

product_id = "P1"
product_df = df[df["product_id"] == product_id].copy()
product_df = product_df.sort_index()
print(product_df.head())
print(product_df.index)

product_df = product_df.fillna(method='ffill')

target = product_df["sales"]
exog_features = ["campaign_type", "discount_ratio", "campaign_duration",
                 "holiday_type", "consumer_confidence_index"]
exog = product_df[exog_features]

# Split train/test (last 30 days as test)
train_target = target[:-30]
test_target = target[-30:]
train_exog = exog[:-30]
test_exog = exog[-30:]

model = SARIMAX(
    train_target,
    exog=train_exog,
    order=(1,1,1),
    seasonal_order=(1,1,1,7),
    enforce_stationarity=False,
    enforce_invertibility=False
)

model_fit = model.fit(disp=False)
print(model_fit.summary())

#Forecast
forecast = model_fit.get_forecast(steps=30, exog=test_exog)
forecast_mean = forecast.predicted_mean
forecast_ci = forecast.conf_int()

rmse = np.sqrt(mean_squared_error(test_target, forecast_mean))
print(f"Test RMSE: {rmse:.2f}")

results = pd.DataFrame({
    "actual": test_target,
    "forecast": forecast_mean
})
print(results.head(15))

plt.figure(figsize=(12,5))
plt.plot(test_target.index, test_target, label="Actual")
plt.plot(test_target.index, forecast_mean, label="Forecast")
plt.fill_between(test_target.index, forecast_ci.iloc[:,0], forecast_ci.iloc[:,1], color='pink', alpha=0.3)
plt.legend()
plt.show()

"""#LSTM Model"""

df = pd.read_csv("Sales_transformed.csv", parse_dates=["DATE"])
df = df.sort_values(["product_id", "DATE"])

product_id = "P1"
product_df = df[df["product_id"] == product_id].set_index("DATE")

product_df = product_df.fillna(method='ffill')

features = ["campaign_type", "discount_ratio", "campaign_duration",
            "holiday_type", "consumer_confidence_index"]
target = "sales"

#Scaling
scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()

X_scaled = scaler_X.fit_transform(product_df[features])
y_scaled = scaler_y.fit_transform(product_df[[target]])

def create_sequences(X, y, seq_length=14, horizon=7):
    X_seq, y_seq = [], []
    for i in range(len(X) - seq_length - horizon + 1):
        X_seq.append(X[i:i+seq_length])
        y_seq.append(y[i+seq_length:i+seq_length+horizon])
    return np.array(X_seq), np.array(y_seq)

SEQ_LENGTH = 14  # look-back 14 days
HORIZON = 7      # predict next 7 days

X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH, HORIZON)

# TRAIN/TEST SPLIT
split = int(len(X_seq) * 0.8)
X_train, X_test = X_seq[:split], X_seq[split:]
y_train, y_test = y_seq[:split], y_seq[split:]

from tensorflow.keras.optimizers import Adam

model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(SEQ_LENGTH, X_train.shape[2])))
model.add(Dropout(0.1))
model.add(Dense(HORIZON))
model.compile(optimizer =Adam(learning_rate=0.001), loss="mse",metrics=["mae"])

# TRAIN MODEL
history = model.fit(X_train, y_train, epochs=50, batch_size=16,
                    validation_split=0.1, verbose=1)

#prediction
y_pred_scaled = model.predict(X_test)
y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, HORIZON))

y_test_actual = scaler_y.inverse_transform(y_test.reshape(-1, HORIZON))

# Compute RMSE per horizon step
for i in range(HORIZON):
    rmse = np.sqrt(mean_squared_error(y_test_actual[:, i], y_pred[:, i]))
    print(f"RMSE t+{i+1}: {rmse:.2f}")

for i in range(HORIZON):
    actual = y_test_actual[0, i]
    forecast = y_pred[0, i]
    print(f"Day t+{i+1}: Actual = {actual:.2f} | Forecast = {forecast:.2f}")

plt.figure(figsize=(14,6))
plt.plot(y_test_actual[:,0], label='Actual t+1')
plt.plot(y_pred[:,0], label='Predicted t+1')
plt.ylabel('Demand')
plt.title('LSTM Forecast - Day 1 of 7-step horizon')
plt.legend()
plt.show()

